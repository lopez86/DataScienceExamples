{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "One common machine learning task is to take user ratings for some product and then try to recommend new products. A common way to do this is by matrix decomposition of a user/product rating matrix. This notebook uses the ratings from the Grouplens MovieLens dataset (https://grouplens.org/datasets/movielens/), which I downloaded from Kaggle at https://www.kaggle.com/rounakbanik/the-movies-dataset. Here, I use the small dataset, which has approximately 100,000 ratings from 700 users. There is a much larger dataset with 26,000,000 ratings available that will likely give much stronger results. I'm just using the smaller one so that things will run quickly. \n",
    "\n",
    "A different approach, such as the Restricted Boltzmann Machine recommender described by Salakhutdinov, Mnih, and Hinton (http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf) might be interesting to test out on the full dataset. This method will tend to have many fewer parameters and will scale better for very large datasets. However, I don't think RBMs are included in the most common neural net frameworks, so you'd have to do the implementation on your own or find something on Github. I may try my own C++ implementation at some point.\n",
    "\n",
    "Returning to the point on scaling, if we use 10 latent features, we will fit for (700 users x 10 features) + (10 features x 9000 movies) = 97,000 matrix elements. For the full dataset, the number is (270,000 users x 10 features) + (10 features x 45,000 movies) = 3.15M elements. There are maybe some interesting implications of these numbers. The smaller dataset has many more movies than users but the opposite is true for the full dataset. We might expect that the latent features could be very different due to this.\n",
    "\n",
    "To create the recommender system, I will use the Spark machine learning library, which has an implementation of an alternating least squares recommender. This notebook includes the following tools techniques:\n",
    "  - PySpark\n",
    "  - Basic mapreduce operations\n",
    "  - Pipeline\n",
    "  - Alternating Least Squares\n",
    "  - k-Fold Cross Validation\n",
    "  - Training-test split\n",
    "  - Hyperparameter grid search\n",
    "  - Recommendations of items for different users\n",
    "  - Recommendations of users for different items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should start Spark. I'm just using 2 cores, but you should increase this to speed things up. I'll also start an SQLContext since I'll be using Spark DataFrames later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "\n",
    "sc = SparkContext('local[2]','Tutorial')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the remaining modules that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "I have the ratings file saved in a text format in my home directory on my Hadoop server. I'll load it as an RDD first. Then, I need to do some cleaning before I can convert it into a Row of numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('ratings_small.csv')\n",
    "def toList(x):\n",
    "    x = x.split(',')\n",
    "    return (x[0],[x[1],x[2]])\n",
    "\n",
    "def has_data(x):\n",
    "    if re.match(r'[A-Za-z]',x[0]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def toNumeric(x):\n",
    "    return Row(userId=int(x[0]),movieId=int(x[1][0]),rating=float(x[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I create a recommender system, I'll look a bit at my data. First, I need to use the functions I created above to get a new RDD with the data that I want. Each entry is now a row with a movie ID, a rating, and a user ID. Everything is numeric here, so I don't know anything else about the users or the movies. There is additional information in other files, but I am not trying to use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=31, rating=2.5, userId=1),\n",
       " Row(movieId=1029, rating=3.0, userId=1),\n",
       " Row(movieId=1061, rating=3.0, userId=1),\n",
       " Row(movieId=1129, rating=2.0, userId=1),\n",
       " Row(movieId=1172, rating=4.0, userId=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = rdd.map(toList).filter(has_data).map(toNumeric)\n",
    "users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate = users.map(lambda x : x[1] )\n",
    "rate.distinct().sortBy(keyfunc=lambda x : x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 distinct ratings, from 0.5 to 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = users.map(lambda x : x[2])\n",
    "user.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 671 distinct users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9066"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = users.map(lambda x : x[0] )\n",
    "movies.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ratings for 9066 distinct movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also just over 100,000 ratings, so an average of 11 ratings per movie and 149 ratings per user. I want to do some aggregations, so I'll convert the dataset into a DataFrame now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = sqlContext.createDataFrame(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Ratings per Movie\n",
      "Mean: 11.03\n",
      "Min: 1\n",
      "Max: 341\n",
      "Std. Dev: 24.05\n"
     ]
    }
   ],
   "source": [
    "gbm = ratings.groupBy(ratings.movieId).count()\n",
    "avg = gbm.agg(functions.avg('count').alias('avg')).collect()[0].avg\n",
    "maxN = gbm.agg(functions.max('count').alias('maxN')).collect()[0].maxN\n",
    "minN = gbm.agg(functions.min('count').alias('minN')).collect()[0].minN\n",
    "std = gbm.agg(functions.stddev('count').alias('std')).collect()[0].std\n",
    "\n",
    "print('# of Ratings per Movie')\n",
    "print('Mean: {:0.4}'.format(avg))\n",
    "print('Min: {}'.format(minN))\n",
    "print('Max: {}'.format(maxN))\n",
    "print('Std. Dev: {:0.4}'.format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Ratings per User\n",
      "Mean: 149.0\n",
      "Min: 20\n",
      "Max: 2391\n",
      "Std. Dev: 231.2\n"
     ]
    }
   ],
   "source": [
    "gbu = ratings.groupBy(ratings.userId).count()\n",
    "avg = gbu.agg(functions.avg('count').alias('avg')).collect()[0].avg\n",
    "maxN = gbu.agg(functions.max('count').alias('maxN')).collect()[0].maxN\n",
    "minN = gbu.agg(functions.min('count').alias('minN')).collect()[0].minN\n",
    "std = gbu.agg(functions.stddev('count').alias('std')).collect()[0].std\n",
    "\n",
    "print('# of Ratings per User')\n",
    "print('Mean: {:0.4}'.format(avg))\n",
    "print('Min: {}'.format(minN))\n",
    "print('Max: {}'.format(maxN))\n",
    "print('Std. Dev: {:0.4}'.format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 0.5, Count: 1101\n",
      "Rating: 1.0, Count: 3326\n",
      "Rating: 1.5, Count: 1687\n",
      "Rating: 2.0, Count: 7271\n",
      "Rating: 2.5, Count: 4449\n",
      "Rating: 3.0, Count: 20064\n",
      "Rating: 3.5, Count: 10538\n",
      "Rating: 4.0, Count: 28750\n",
      "Rating: 4.5, Count: 7723\n",
      "Rating: 5.0, Count: 15095\n",
      "Mean Rating: 3.54\n",
      "Std. Dev: 1.06\n"
     ]
    }
   ],
   "source": [
    "gbr = ratings.groupBy(ratings.rating).count() \\\n",
    "             .sort(functions.col('rating')).collect()\n",
    "mean = 0\n",
    "tot = 0\n",
    "std = 0\n",
    "for rat in gbr:\n",
    "    print('Rating: {}, Count: {}'.format(rat.rating,rat['count']))\n",
    "    c = rat['count']\n",
    "    mean += c * rat.rating\n",
    "    std += c * rat.rating*rat.rating\n",
    "    tot += c\n",
    "mean /= tot\n",
    "std /= tot\n",
    "std = np.sqrt(tot/(tot-1))* np.sqrt(std - mean*mean)\n",
    "print('Mean Rating: {:0.3}'.format(mean))\n",
    "print('Std. Dev: {:0.3}'.format(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these aggregations we see several things.\n",
    "\n",
    "First, the counts for both movies and users are highly right-skewed. Movies have as few as a single rating or as many as several hundred. When we run cross validation or use a test set, we expect that the movies with very few ratings will not generalize well. In many cases, they will not even be used in the training. Users have as few as 20 or as many as thousands of ratings, so at least each user should have several movies in a random split.\n",
    "\n",
    "The ratings data shows that the ratings have a fairly high average, around 3.5 of 5, and a standard deviation of 1.06. So, if we just guess an average of 3.54 for all ratings, we'd expect an RMSE of 1.06. We have to beat this if our recommender system is doing anything useful.\n",
    "\n",
    "You can also see that users are much more likely to give integer ratings than half-integer ratings, and there are few very low ratings. Movie watchers tend to only pick movies that they want to see, so it may not be surprising that there are so few ratings where users truly hated a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System\n",
    "\n",
    "The recommender system that I use here is the Alternating Least Squares implementation in the Spark ML package. Suppose we have a matrix $\\mathbf{R}$ where $R_{ij}$ corresponds to a rating of movie $i$ from user $j$. Then, we can attempt to decompose $\\mathbf{R}$ as:\n",
    "\n",
    "$$ \\mathbf{R} = \\mathbf{M}\\mathbf{U} $$\n",
    "\n",
    "where $\\mathbf{M}$ is an $m$ by $k$ matrix and $\\mathbf{U}$ is a $k$ by $n$ matrix. Here, $m$ is the number of movies, $n$ is the number of users, and $k$ is a number of latent features that we wish to learn.\n",
    "\n",
    "If the number of latent features is large, we can get an exact representation of $\\mathbf{R}$. However, this would not be very useful. $\\mathbf{R}$ is a sparse matrix in most cases and we wish to guess values for the unfilled elements. What we instead want to do is use a small number of latent features so that the number of elements in $\\mathbf{R}$ is much greater than the number of elements in $\\mathbf{M}$ and $\\mathbf{U}$. Now, we can try to fit by minimizing the squared error between the MU decomposition and R:\n",
    "\n",
    "$$ E = \\frac{1}{2N}\\sum\\limits_{r=0}^N \\left(R_{i_r j_r} - M_{i_r k}U_{k j_r}\\right)^2 $$\n",
    "\n",
    "where $N$ is the total number of input ratings and $r$ is an index for each rating. There is a problem here, though. This error function is quartic in our fit parameters. We can't guarantee that this is a convex function, so minimization could find a local but not global minimum. \n",
    "\n",
    "This is where alternating least squares comes in. We can run an iterative process to find a solution. To do this, we can fix either U or M and minimize the error for just one of our matrices. This would be a quadratic function, so methods such as gradient descent are guaranteed to find the solution if run with reasonable parameters. If the matrix is small enough, we could even just get an exact solution using matrix inversion.\n",
    "\n",
    "Once we do one fit, we fix the newly-fit matrix and fit again for the other matrix. After some number of iterations, we stop this and look at the result. So, we are running least squares many times by alternating which of the two matrices we are optimizing.\n",
    "\n",
    "To actually implement this, we first want to run a training/test split on our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test) = ratings.randomSplit([0.8,0.2],seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS object in Spark has several different parameters. I am leaving the rank (the number of latent features) at 10, but we realistically would want to optimize this. There is also a regularization parameter, and the number of iterations. I am just setting the number of iterations to 10 and optimizing only the regularization. I am also setting the coldStartStrategy to \"drop.\" This sort of recommender needs to be fit on all users, so if a user is absent from the training set, the output is nan. This will suppress those values. The RBM recommender I mentioned at the beginning does not have this issue, which is one thing that makes it an intriguing possibility.\n",
    "\n",
    "If you really want to get recommendations for a new user without retraining, you could try to identify the most similar users (maybe by using cosine similarity or even unsupervised clustering) and calculate ratings from those users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10,regParam=0.2,userCol='userId',\n",
    "          itemCol='movieId',ratingCol='rating',\n",
    "          coldStartStrategy='drop')\n",
    "\n",
    "pipeline = Pipeline(stages=[als])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "param = ParamGridBuilder().addGrid(als.regParam,[0.01,0.03,0.05,0.1,0.2,0.3,0.5]).build()\n",
    "cv = CrossValidator(estimator=pipeline,estimatorParamMaps=param,\n",
    "                    evaluator=evaluator,numFolds=5)\n",
    "cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now fit a cross validation model and trained it on my training set. First, I should see what parameter I chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Param: 0.01, RMSE: 1.23\n",
      "Reg Param: 0.03, RMSE: 1.1\n",
      "Reg Param: 0.05, RMSE: 1.03\n",
      "Reg Param: 0.1, RMSE: 0.942\n",
      "Reg Param: 0.2, RMSE: 0.917\n",
      "Reg Param: 0.3, RMSE: 0.944\n",
      "Reg Param: 0.5, RMSE: 1.03\n"
     ]
    }
   ],
   "source": [
    "for par,score in zip(param,cvModel.avgMetrics):\n",
    "    print(\"Reg Param: {}, RMSE: {:0.3}\".format(list(par.values())[0],score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that a regularization parameter of 0.2 gave the optimal result. With this amount of regularization, the RMSE error shows that I can accurately estimate users' scores to less than one point (of 5). Now, we can run on our test set to see how the result compares on new data from the same users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.904858797400974\n",
      "Trivial model RMSE = 1.056236423550511\n"
     ]
    }
   ],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "predictions = predictions.withColumn('trivial',functions.lit(3.54))\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"trivial\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print('Trivial model RMSE = ' + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ther result on the test set is very close to the cross-validation result. This shows that our model is generalizing well, so it should be effective to use for making recommendations. Furthermore, the error is about 15% less than what we would get if we just always pick the mean rating. So, it's not amazing, but we have at least improved upon a trivial model using a fairly small data set. Now, we should see what the predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=463, rating=4.0, userId=534, prediction=3.5110599994659424, trivial=3.54),\n",
       " Row(movieId=471, rating=3.0, userId=588, prediction=3.7625296115875244, trivial=3.54),\n",
       " Row(movieId=471, rating=5.0, userId=460, prediction=3.823929786682129, trivial=3.54),\n",
       " Row(movieId=471, rating=3.0, userId=350, prediction=3.84967041015625, trivial=3.54),\n",
       " Row(movieId=471, rating=5.0, userId=285, prediction=3.5324244499206543, trivial=3.54),\n",
       " Row(movieId=471, rating=3.0, userId=440, prediction=3.685539722442627, trivial=3.54),\n",
       " Row(movieId=471, rating=3.0, userId=491, prediction=3.87979793548584, trivial=3.54),\n",
       " Row(movieId=471, rating=4.0, userId=659, prediction=3.603774070739746, trivial=3.54),\n",
       " Row(movieId=471, rating=4.0, userId=487, prediction=3.867652654647827, trivial=3.54),\n",
       " Row(movieId=471, rating=4.0, userId=105, prediction=3.5243277549743652, trivial=3.54),\n",
       " Row(movieId=471, rating=4.0, userId=529, prediction=3.451523542404175, trivial=3.54),\n",
       " Row(movieId=471, rating=4.0, userId=30, prediction=3.9350345134735107, trivial=3.54),\n",
       " Row(movieId=471, rating=0.5, userId=311, prediction=2.9447827339172363, trivial=3.54),\n",
       " Row(movieId=1088, rating=4.0, userId=500, prediction=3.0502870082855225, trivial=3.54),\n",
       " Row(movieId=1088, rating=4.0, userId=57, prediction=3.690937042236328, trivial=3.54),\n",
       " Row(movieId=1088, rating=2.0, userId=607, prediction=3.3273096084594727, trivial=3.54),\n",
       " Row(movieId=1088, rating=2.0, userId=15, prediction=2.1446213722229004, trivial=3.54),\n",
       " Row(movieId=1088, rating=2.0, userId=262, prediction=2.0677988529205322, trivial=3.54),\n",
       " Row(movieId=1088, rating=5.0, userId=418, prediction=3.5889246463775635, trivial=3.54),\n",
       " Row(movieId=1088, rating=4.0, userId=160, prediction=3.756554126739502, trivial=3.54)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, each prediction gives a movie and user ID, the actual rating, and the predicted rating. From here, we can get the top recommendations for each user.\n",
    "\n",
    "I'll look at the top 10 recommended movies for ten users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 471\n",
      "\tMovie: 067504, Rating: 5.21\n",
      "\tMovie: 132333, Rating: 4.98\n",
      "\tMovie: 065037, Rating: 4.93\n",
      "\tMovie: 008535, Rating: 4.86\n",
      "\tMovie: 144976, Rating: 4.85\n",
      "\tMovie: 005765, Rating: 4.83\n",
      "\tMovie: 004630, Rating: 4.83\n",
      "\tMovie: 106471, Rating: 4.82\n",
      "\tMovie: 003030, Rating: 4.77\n",
      "\tMovie: 093320, Rating: 4.75\n",
      "User: 463\n",
      "\tMovie: 067504, Rating: 5.16\n",
      "\tMovie: 072647, Rating: 4.64\n",
      "\tMovie: 004405, Rating: 4.64\n",
      "\tMovie: 031547, Rating: 4.64\n",
      "\tMovie: 005059, Rating: 4.64\n",
      "\tMovie: 025764, Rating: 4.64\n",
      "\tMovie: 007074, Rating: 4.64\n",
      "\tMovie: 080599, Rating: 4.64\n",
      "\tMovie: 008535, Rating: 4.62\n",
      "\tMovie: 132333, Rating: 4.62\n",
      "User: 496\n",
      "\tMovie: 065037, Rating: 5.38\n",
      "\tMovie: 067504, Rating: 5.38\n",
      "\tMovie: 008535, Rating: 5.34\n",
      "\tMovie: 008908, Rating: 5.33\n",
      "\tMovie: 090061, Rating: 5.32\n",
      "\tMovie: 132333, Rating: 5.22\n",
      "\tMovie: 106471, Rating: 5.19\n",
      "\tMovie: 144976, Rating: 5.18\n",
      "\tMovie: 004630, Rating: 5.17\n",
      "\tMovie: 005765, Rating: 5.17\n",
      "User: 148\n",
      "\tMovie: 067504, Rating: 5.83\n",
      "\tMovie: 031547, Rating: 5.25\n",
      "\tMovie: 005059, Rating: 5.25\n",
      "\tMovie: 072647, Rating: 5.25\n",
      "\tMovie: 007074, Rating: 5.25\n",
      "\tMovie: 025764, Rating: 5.25\n",
      "\tMovie: 004405, Rating: 5.25\n",
      "\tMovie: 080599, Rating: 5.25\n",
      "\tMovie: 008535, Rating: 5.21\n",
      "\tMovie: 144976, Rating: 5.17\n",
      "User: 540\n",
      "\tMovie: 090061, Rating: 5.46\n",
      "\tMovie: 062718, Rating: 5.39\n",
      "\tMovie: 065037, Rating: 5.29\n",
      "\tMovie: 007669, Rating: 5.18\n",
      "\tMovie: 006918, Rating: 5.13\n",
      "\tMovie: 008955, Rating: 5.13\n",
      "\tMovie: 008535, Rating: 5.09\n",
      "\tMovie: 003746, Rating: 5.07\n",
      "\tMovie: 008908, Rating: 5.07\n",
      "\tMovie: 132333, Rating: 5.07\n",
      "User: 392\n",
      "\tMovie: 003216, Rating: 4.47\n",
      "\tMovie: 026974, Rating: 4.47\n",
      "\tMovie: 040412, Rating: 4.47\n",
      "\tMovie: 092494, Rating: 4.47\n",
      "\tMovie: 067504, Rating: 4.47\n",
      "\tMovie: 065037, Rating: 4.13\n",
      "\tMovie: 008955, Rating: 4.12\n",
      "\tMovie: 006918, Rating: 4.12\n",
      "\tMovie: 003746, Rating: 4.12\n",
      "\tMovie: 093320, Rating: 4.06\n",
      "User: 243\n",
      "\tMovie: 067504, Rating: 5.19\n",
      "\tMovie: 092494, Rating: 4.76\n",
      "\tMovie: 026974, Rating: 4.76\n",
      "\tMovie: 040412, Rating: 4.76\n",
      "\tMovie: 003216, Rating: 4.76\n",
      "\tMovie: 065037, Rating: 4.67\n",
      "\tMovie: 007074, Rating: 4.67\n",
      "\tMovie: 072647, Rating: 4.67\n",
      "\tMovie: 004405, Rating: 4.67\n",
      "\tMovie: 025764, Rating: 4.67\n",
      "User: 623\n",
      "\tMovie: 067504, Rating: 5.75\n",
      "\tMovie: 026974, Rating: 5.36\n",
      "\tMovie: 003216, Rating: 5.36\n",
      "\tMovie: 092494, Rating: 5.36\n",
      "\tMovie: 040412, Rating: 5.36\n",
      "\tMovie: 065037, Rating: 5.25\n",
      "\tMovie: 008955, Rating: 5.22\n",
      "\tMovie: 006918, Rating: 5.22\n",
      "\tMovie: 144976, Rating: 5.19\n",
      "\tMovie: 007074, Rating: 5.17\n",
      "User: 31\n",
      "\tMovie: 067504, Rating: 5.59\n",
      "\tMovie: 026974, Rating: 5.2\n",
      "\tMovie: 040412, Rating: 5.2\n",
      "\tMovie: 003216, Rating: 5.2\n",
      "\tMovie: 092494, Rating: 5.2\n",
      "\tMovie: 065037, Rating: 5.18\n",
      "\tMovie: 144976, Rating: 5.1\n",
      "\tMovie: 008535, Rating: 5.09\n",
      "\tMovie: 132333, Rating: 5.09\n",
      "\tMovie: 008955, Rating: 5.06\n",
      "User: 516\n",
      "\tMovie: 067504, Rating: 5.07\n",
      "\tMovie: 065037, Rating: 4.84\n",
      "\tMovie: 008535, Rating: 4.81\n",
      "\tMovie: 132333, Rating: 4.76\n",
      "\tMovie: 005765, Rating: 4.76\n",
      "\tMovie: 004630, Rating: 4.76\n",
      "\tMovie: 144976, Rating: 4.75\n",
      "\tMovie: 090061, Rating: 4.73\n",
      "\tMovie: 106471, Rating: 4.68\n",
      "\tMovie: 008955, Rating: 4.66\n"
     ]
    }
   ],
   "source": [
    "userRecs = cvModel.bestModel.stages[0].recommendForAllUsers(10)\n",
    "recs = userRecs.take(10)\n",
    "for row in recs:\n",
    "    print(\"User: {}\".format(row.userId))\n",
    "    for movie in row.recommendations:\n",
    "        print('\\tMovie: {:06}, Rating: {:0.3}'.format(movie.movieId,movie.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually see here that many of top the predicted ratings are greater than 5, which is an impossible score. This isn't necessarily a terrible thing, as it gives us some way to order even very highly-rated movies. If we reduce these back to 5, we would see an improvement in our error. However, if our regularization is very small, we start seeing some very large predictions such as a score of 9. We would expect the same at the low end, where the worst movies for each user may well drop below the minimum score of 0.5.\n",
    "\n",
    "Finally, we can recommend users to each item. Mathematically, there is no difference but there may also be good reasons to look at this. Perhaps you have some stock of an item that you need to sell. Even if it's not anyone's top recommended item, you could still recommend it to the top users to increase sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: 1580\n",
      "\tUser: 46, Rating: 4.83\n",
      "\tUser: 113, Rating: 4.76\n",
      "\tUser: 543, Rating: 4.52\n",
      "\tUser: 287, Rating: 4.44\n",
      "\tUser: 656, Rating: 4.41\n",
      "\tUser: 89, Rating: 4.39\n",
      "\tUser: 145, Rating: 4.38\n",
      "\tUser: 646, Rating: 4.35\n",
      "\tUser: 517, Rating: 4.33\n",
      "\tUser: 464, Rating: 4.32\n",
      "Movie: 5300\n",
      "\tUser: 227, Rating: 5.74\n",
      "\tUser: 46, Rating: 5.72\n",
      "\tUser: 113, Rating: 5.56\n",
      "\tUser: 123, Rating: 5.43\n",
      "\tUser: 290, Rating: 5.32\n",
      "\tUser: 89, Rating: 5.23\n",
      "\tUser: 78, Rating: 5.21\n",
      "\tUser: 592, Rating: 5.18\n",
      "\tUser: 177, Rating: 5.16\n",
      "\tUser: 145, Rating: 5.14\n",
      "Movie: 6620\n",
      "\tUser: 123, Rating: 4.54\n",
      "\tUser: 357, Rating: 4.53\n",
      "\tUser: 113, Rating: 4.41\n",
      "\tUser: 401, Rating: 4.39\n",
      "\tUser: 156, Rating: 4.37\n",
      "\tUser: 242, Rating: 4.36\n",
      "\tUser: 280, Rating: 4.36\n",
      "\tUser: 298, Rating: 4.35\n",
      "\tUser: 4, Rating: 4.34\n",
      "\tUser: 264, Rating: 4.34\n",
      "Movie: 32460\n",
      "\tUser: 46, Rating: 4.88\n",
      "\tUser: 113, Rating: 4.84\n",
      "\tUser: 298, Rating: 4.81\n",
      "\tUser: 89, Rating: 4.77\n",
      "\tUser: 401, Rating: 4.75\n",
      "\tUser: 443, Rating: 4.73\n",
      "\tUser: 287, Rating: 4.72\n",
      "\tUser: 357, Rating: 4.7\n",
      "\tUser: 543, Rating: 4.65\n",
      "\tUser: 473, Rating: 4.65\n",
      "Movie: 54190\n",
      "\tUser: 357, Rating: 4.72\n",
      "\tUser: 113, Rating: 4.71\n",
      "\tUser: 298, Rating: 4.68\n",
      "\tUser: 98, Rating: 4.64\n",
      "\tUser: 264, Rating: 4.63\n",
      "\tUser: 46, Rating: 4.61\n",
      "\tUser: 464, Rating: 4.61\n",
      "\tUser: 89, Rating: 4.57\n",
      "\tUser: 242, Rating: 4.57\n",
      "\tUser: 473, Rating: 4.56\n",
      "Movie: 471\n",
      "\tUser: 401, Rating: 4.64\n",
      "\tUser: 357, Rating: 4.62\n",
      "\tUser: 577, Rating: 4.56\n",
      "\tUser: 4, Rating: 4.55\n",
      "\tUser: 46, Rating: 4.54\n",
      "\tUser: 443, Rating: 4.54\n",
      "\tUser: 242, Rating: 4.53\n",
      "\tUser: 280, Rating: 4.51\n",
      "\tUser: 298, Rating: 4.48\n",
      "\tUser: 153, Rating: 4.44\n",
      "Movie: 1591\n",
      "\tUser: 46, Rating: 3.76\n",
      "\tUser: 113, Rating: 3.57\n",
      "\tUser: 145, Rating: 3.51\n",
      "\tUser: 546, Rating: 3.47\n",
      "\tUser: 290, Rating: 3.44\n",
      "\tUser: 89, Rating: 3.43\n",
      "\tUser: 287, Rating: 3.39\n",
      "\tUser: 543, Rating: 3.33\n",
      "\tUser: 78, Rating: 3.31\n",
      "\tUser: 227, Rating: 3.29\n",
      "Movie: 1342\n",
      "\tUser: 113, Rating: 3.52\n",
      "\tUser: 357, Rating: 3.51\n",
      "\tUser: 4, Rating: 3.46\n",
      "\tUser: 123, Rating: 3.46\n",
      "\tUser: 564, Rating: 3.45\n",
      "\tUser: 577, Rating: 3.42\n",
      "\tUser: 401, Rating: 3.41\n",
      "\tUser: 46, Rating: 3.41\n",
      "\tUser: 242, Rating: 3.39\n",
      "\tUser: 637, Rating: 3.36\n",
      "Movie: 2122\n",
      "\tUser: 46, Rating: 3.56\n",
      "\tUser: 113, Rating: 3.46\n",
      "\tUser: 227, Rating: 3.16\n",
      "\tUser: 398, Rating: 3.12\n",
      "\tUser: 70, Rating: 3.08\n",
      "\tUser: 564, Rating: 2.98\n",
      "\tUser: 652, Rating: 2.96\n",
      "\tUser: 145, Rating: 2.91\n",
      "\tUser: 110, Rating: 2.91\n",
      "\tUser: 543, Rating: 2.9\n",
      "Movie: 2142\n",
      "\tUser: 46, Rating: 4.4\n",
      "\tUser: 113, Rating: 4.08\n",
      "\tUser: 290, Rating: 3.97\n",
      "\tUser: 543, Rating: 3.96\n",
      "\tUser: 89, Rating: 3.95\n",
      "\tUser: 401, Rating: 3.94\n",
      "\tUser: 656, Rating: 3.93\n",
      "\tUser: 287, Rating: 3.92\n",
      "\tUser: 443, Rating: 3.84\n",
      "\tUser: 123, Rating: 3.84\n"
     ]
    }
   ],
   "source": [
    "itemRecs = cvModel.bestModel.stages[0].recommendForAllItems(10)\n",
    "recs = itemRecs.take(10)\n",
    "for row in recs:\n",
    "    print(\"Movie: {}\".format(row.movieId))\n",
    "    for user in row.recommendations:\n",
    "        print('\\tUser: {}, Rating: {:0.3}'.format(user.userId,user.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, I've used collaborative filtering in Spark to produce movie recommendations based on user rating data. The model used a fairly small dataset but was able to achieve a substantial improvement over what we would expect from a trivial rating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
