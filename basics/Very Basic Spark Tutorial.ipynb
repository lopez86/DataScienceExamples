{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here is a brief intro to some of the basics of using PySpark. You'll need to have Hadoop, Spark, and an ssh server available on your machine. I'm running on a laptop, so I don't normally have the ssh server or Hadoop running. If you aren't used to Linux, you can start ssh with a command like this (you may need to sudo these):\n",
    "```bash\n",
    "/etc/init.d/ssh start\n",
    "```\n",
    "You also want to start the Hadoop server unless you're running Spark in standalone mode.\n",
    "```bash\n",
    "start-dfs.sh\n",
    "```\n",
    "Finally, once Hadoop is running you can do basic things like copy files to your home directory, list files, etc like this:\n",
    "```bash\n",
    "hdfs dfs -cp file:$PATH .\n",
    "```\n",
    "\n",
    "From here, the first thing to do is to create the SparkContext. Note that if you run this twice without restarting the kernel, you will get an error saying that there can only be one SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local[2]').setAppName('Tutorial')\n",
    "sc = SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a Dataset\n",
    "\n",
    "Now that everything is running, we can load in some data. I've placed the text of Moby Dick into my Hadoop home directory. This is from Project Gutenberg and is the version currently included as part of the NLTK Gutenberg corpus. Since this is just a text file, we can create a Spark RDD by just calling textFile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.textFile('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in the dataset represents a line. So how many lines are there? We can just run the count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not very interesting. It turns out that the main text starts right at line 500. Let's grab the first 550 lines and then print the first 50 lines of the actual text. You may remember that the first line of Moby Dick is \"Call me Ishmael.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER 1\n",
      "\n",
      "Loomings.\n",
      "\n",
      "\n",
      "Call me Ishmael.  Some years ago--never mind how long\n",
      "precisely--having little or no money in my purse, and nothing\n",
      "particular to interest me on shore, I thought I would sail about a\n",
      "little and see the watery part of the world.  It is a way I have of\n",
      "driving off the spleen and regulating the circulation.  Whenever I\n",
      "find myself growing grim about the mouth; whenever it is a damp,\n",
      "drizzly November in my soul; whenever I find myself involuntarily\n",
      "pausing before coffin warehouses, and bringing up the rear of every\n",
      "funeral I meet; and especially whenever my hypos get such an upper\n",
      "hand of me, that it requires a strong moral principle to prevent me\n",
      "from deliberately stepping into the street, and methodically knocking\n",
      "people's hats off--then, I account it high time to get to sea as soon\n",
      "as I can.  This is my substitute for pistol and ball.  With a\n",
      "philosophical flourish Cato throws himself upon his sword; I quietly\n",
      "take to the ship.  There is nothing surprising in this.  If they but\n",
      "knew it, almost all men in their degree, some time or other, cherish\n",
      "very nearly the same feelings towards the ocean with me.\n",
      "\n",
      "There now is your insular city of the Manhattoes, belted round by\n",
      "wharves as Indian isles by coral reefs--commerce surrounds it with\n",
      "her surf.  Right and left, the streets take you waterward.  Its\n",
      "extreme downtown is the battery, where that noble mole is washed by\n",
      "waves, and cooled by breezes, which a few hours previous were out of\n",
      "sight of land.  Look at the crowds of water-gazers there.\n",
      "\n",
      "Circumambulate the city of a dreamy Sabbath afternoon.  Go from\n",
      "Corlears Hook to Coenties Slip, and from thence, by Whitehall,\n",
      "northward.  What do you see?--Posted like silent sentinels all around\n",
      "the town, stand thousands upon thousands of mortal men fixed in ocean\n",
      "reveries.  Some leaning against the spiles; some seated upon the\n",
      "pier-heads; some looking over the bulwarks of ships from China; some\n",
      "high aloft in the rigging, as if striving to get a still better\n",
      "seaward peep.  But these are all landsmen; of week days pent up in\n",
      "lath and plaster--tied to counters, nailed to benches, clinched to\n",
      "desks.  How then is this?  Are the green fields gone?  What do they\n",
      "here?\n",
      "\n",
      "But look! here come more crowds, pacing straight for the water, and\n",
      "seemingly bound for a dive.  Strange!  Nothing will content them but\n",
      "the extremest limit of the land; loitering under the shady lee of\n",
      "yonder warehouses will not suffice.  No.  They must get just as nigh\n",
      "the water as they possibly can without falling in.  And there they\n",
      "stand--miles of them--leagues.  Inlanders all, they come from lanes\n",
      "and alleys, streets and avenues--north, east, south, and west.  Yet\n"
     ]
    }
   ],
   "source": [
    "lines = df.take(550)\n",
    "for i in range(500,550):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words in Moby Dick\n",
    "\n",
    "We already got the number of lines, but that is dependent on things like the intended font and page width. It would be better to count things like the number of words. This is probably the most common task in the Spark introductions that I've seen. I'll do something a bit more complicated than what is found in the introduction on the Spark website.\n",
    "\n",
    "Because this is a prose text, there is a lot of punctuation. Here, I'll first remove all non alphanumeric characters and then get a count of each distinct word, where a \"word\" in this context is just a set of consecutive non-whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "allwords = df.map(lambda x : re.sub(r'\\W',' ',x)). \\\n",
    "                flatMap(lambda line: line.split()).\\\n",
    "                filter(lambda x: x!='')\n",
    "\n",
    "counts = allwords.map(lambda word: (word,1)).\\\n",
    "                  reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I've split the operations into two parts. One gets a list of all words, while the other takes the words and gets a count for each unique word. I actually haven't calculated anything. I'll need to call another function to tell Spark to run these functions. But, what are the functions that I've called here?\n",
    "\n",
    "```python\n",
    "map()\n",
    "```\n",
    "This may be familiar to you from Pandas or even from regular Python code.\n",
    "Map loops over all entries in the dataset, applies the given function, and returns a new dataset with the results.\n",
    "\n",
    "```python\n",
    "flatMap()\n",
    "```\n",
    "This function is much like map() except it flattens things like lists into a series of individual entries.\n",
    "\n",
    "```python\n",
    "filter()\n",
    "```\n",
    "This may also be familiar. It gets a new dataset after removing entries failing the given function.\n",
    "\n",
    "```python\n",
    "reduceByKey()\n",
    "```\n",
    "Finally, there's reduceByKey(). The last map() statement turns each word into a (key,value) pair. reduceByKey() groups together all entries with the same key and then runs some function. In the lambda function that I've defined, I add the new value (y) to the current sum (x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of words found: 218621\n",
      "Total # of distinct words found: 19226\n",
      "Another distinct count: 19226\n"
     ]
    }
   ],
   "source": [
    "totalwords = allwords.count()\n",
    "print(\"Total # of words found: {}\".format(totalwords))\n",
    "distinctwords = counts.count()\n",
    "print('Total # of distinct words found: {}'.format(distinctwords))\n",
    "# Alternative distinct counter:\n",
    "print('Another distinct count: {}'.format(allwords.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that there are 218,621 distinct \"words\" here and 19,226 distinct words. Of course, this isn't really the same as if we went through the text by hand. I haven't attempted to correct for capitalization. I also haven't tried to correct contractions to their original words. Something like \"can't\" will be corrected to \"can t\" here, which will be counted as two words.\n",
    "\n",
    "# More Operations\n",
    "\n",
    "What if we want to get the most common word? We'll just define a comparison function and run reduce()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 13721)\n"
     ]
    }
   ],
   "source": [
    "def get_max(cur_max,newval):\n",
    "    if cur_max[1]>=newval[1]:\n",
    "        return cur_max\n",
    "    return newval\n",
    "top = counts.reduce(get_max )\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort the results by defining a function that returns a value to be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Consumptive', 1),\n",
       " ('lexicons', 1),\n",
       " ('mockingly', 1),\n",
       " ('flags', 1),\n",
       " ('HVAL', 1),\n",
       " ('Dut', 1),\n",
       " ('Ger', 1),\n",
       " ('WALW', 1),\n",
       " ('RICHARDSON', 1),\n",
       " ('LATIN', 1),\n",
       " ('WHOEL', 1),\n",
       " ('SAXON', 1),\n",
       " ('HWAL', 1),\n",
       " ('SWEDISH', 1),\n",
       " ('ICELANDIC', 1),\n",
       " ('Librarian', 1),\n",
       " ('painstaking', 1),\n",
       " ('burrower', 1),\n",
       " ('grub', 1),\n",
       " ('stalls', 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedData = counts.sortBy(lambda x : x[1],ascending=True)\n",
    "sortedData.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get an unsorted list of some of the terms that appear only once. Many of these are capitalized or even in all caps and some aren't even English words, so we see that further analysis could change this list significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 13721),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " ('in', 3916),\n",
       " ('that', 2982),\n",
       " ('his', 2459),\n",
       " ('it', 2209),\n",
       " ('I', 2124),\n",
       " ('s', 1739),\n",
       " ('is', 1695),\n",
       " ('he', 1661),\n",
       " ('with', 1659),\n",
       " ('was', 1632),\n",
       " ('as', 1620),\n",
       " ('all', 1462),\n",
       " ('for', 1414),\n",
       " ('this', 1280),\n",
       " ('at', 1231)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedData = counts.sortBy(lambda x: x[1],ascending=False)\n",
    "sortedData.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, many of the most common words are basic words such as pronouns, articles, conjunctions, and prepositions. We can also sort the words by key to get an alphabetical list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000', 20),\n",
       " ('1', 2),\n",
       " ('10', 4),\n",
       " ('100', 1),\n",
       " ('101', 1),\n",
       " ('102', 1),\n",
       " ('103', 1),\n",
       " ('104', 1),\n",
       " ('105', 1),\n",
       " ('106', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zoology', 1),\n",
       " ('zones', 3),\n",
       " ('zoned', 1),\n",
       " ('zone', 5),\n",
       " ('zodiac', 3),\n",
       " ('zig', 1),\n",
       " ('zephyr', 1),\n",
       " ('zeal', 2),\n",
       " ('zay', 1),\n",
       " ('zag', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sortByKey(ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that we have three forms of the word \"zone.\" For many applications, we might want to run a stemmer or lemmatizer, which should combine these into a single entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
